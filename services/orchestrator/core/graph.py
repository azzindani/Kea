"""
LangGraph Research State Machine.

Implements the cyclic research flow with checkpointing.
This is the PRODUCTION version that uses real LLM and MCP implementations.
"""

from __future__ import annotations

from datetime import datetime
from enum import Enum
from typing import Any, Literal, TypedDict

from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field

from shared.schemas import ResearchState, ResearchStatus, QueryPath
from shared.logging import get_logger

# Import real implementations
from services.orchestrator.nodes.planner import planner_node as real_planner_node
from services.orchestrator.nodes.keeper import keeper_node as real_keeper_node
from services.orchestrator.agents.generator import GeneratorAgent
from services.orchestrator.agents.critic import CriticAgent
from services.orchestrator.agents.judge import JudgeAgent
from services.orchestrator.core.router import IntentionRouter

# NEW: Context pool and code generator for dynamic data flow
from services.orchestrator.core.context_pool import get_context_pool, reset_context_pool
from services.orchestrator.agents.code_generator import generate_fallback_code, generate_python_code


logger = get_logger(__name__)


# ============================================================================
# Node Names
# ============================================================================

class NodeName(str, Enum):
    """LangGraph node names."""
    ROUTER = "router"
    PLANNER = "planner"
    RESEARCHER = "researcher"
    KEEPER = "keeper"
    GENERATOR = "generator"
    CRITIC = "critic"
    JUDGE = "judge"
    SYNTHESIZER = "synthesizer"


# ============================================================================
# Graph State (TypedDict for LangGraph)
# ============================================================================

class GraphState(TypedDict, total=False):
    """State passed between LangGraph nodes."""
    # Core
    job_id: str
    query: str
    path: str
    status: str
    
    # Planning
    sub_queries: list[str]
    hypotheses: list[str]
    execution_plan: dict  # Execution plan from planner
    
    # Execution
    facts: list[dict]
    sources: list[dict]
    artifacts: list[str]
    tool_invocations: list[dict]  # Record of tool calls
    
    # Consensus
    generator_output: str
    critic_feedback: str
    judge_verdict: str
    
    # Output
    report: str
    confidence: float
    
    # Control
    iteration: int
    max_iterations: int
    should_continue: bool
    error: str | None


# ============================================================================
# Node Implementations (VERBOSE MODE)
# ============================================================================

async def router_node(state: GraphState) -> GraphState:
    """Route query to appropriate execution path."""
    query = state.get("query", "")
    
    logger.info("\n" + "="*70)
    logger.info("ðŸ“ STEP 1: ROUTER NODE")
    logger.info("="*70)
    logger.info(f"Query: {query[:200]}...")
    
    router = IntentionRouter()
    context = {
        "prior_research": state.get("prior_research"),
        "data_to_verify": state.get("data_to_verify"),
    }
    
    path = await router.route(query, context)
    
    logger.info(f"âœ… Selected Path: {path}")
    logger.info(f"   A = Memory Fork | B = Shadow Lab | C = Grand Synthesis | D = Deep Research")
    logger.info("="*70 + "\n")
    
    logger.info(f"Router: Selected path {path}")
    
    return {
        **state,
        "path": path,
        "status": "running",
    }


async def planner_node(state: GraphState) -> GraphState:
    """Decompose query into sub-queries, hypotheses, and execution plan using real LLM."""
    logger.info("\n" + "="*70)
    logger.info("ðŸ“ STEP 2: PLANNER NODE (LLM Call)")
    logger.info("="*70)
    logger.info(f"Query: {state.get('query', '')[:200]}...")
    logger.info("-"*70)
    logger.info("Calling LLM to decompose query into sub-queries and execution plan...")
    
    # Call real planner that uses OpenRouter LLM
    updated_state = await real_planner_node(dict(state))
    
    sub_queries = updated_state.get("sub_queries", [])
    hypotheses = updated_state.get("hypotheses", [])
    execution_plan = updated_state.get("execution_plan", {})
    
    logger.info(f"\nâœ… Sub-Queries Generated ({len(sub_queries)}):")
    for i, sq in enumerate(sub_queries, 1):
        logger.info(f"   {i}. {sq}")
    
    logger.info(f"\nâœ… Hypotheses Generated ({len(hypotheses)}):")
    for i, h in enumerate(hypotheses, 1):
        logger.info(f"   {i}. {h}")
    
    # Log execution plan
    micro_tasks = execution_plan.get("micro_tasks", [])
    if micro_tasks:
        logger.info(f"\nâœ… Execution Plan Generated ({len(micro_tasks)} micro-tasks):")
        for task in micro_tasks:
            logger.info(f"   [{task.get('task_id')}] {task.get('tool')}: {task.get('description', '')[:60]}...")
        logger.info(f"   Phases: {execution_plan.get('phases', [])}")
    
    logger.info("="*70 + "\n")
    
    # Increment iteration
    updated_state["iteration"] = state.get("iteration", 0) + 1
    
    return {**state, **updated_state}

async def researcher_node(state: GraphState) -> GraphState:
    """Execute research using execution plan with dynamic tool routing and PARALLEL execution."""
    iteration = state.get("iteration", 1)
    
    logger.info("\n" + "="*70)
    logger.info(f"ðŸ“ STEP 3: RESEARCHER NODE (Iteration {iteration}) - Parallel Tool Execution")
    logger.info("="*70)
    
    execution_plan = state.get("execution_plan", {})
    micro_tasks = execution_plan.get("micro_tasks", [])
    sub_queries = state.get("sub_queries", [])
    query = state.get("query", "")
    facts = state.get("facts", [])
    sources = state.get("sources", [])
    tool_invocations = state.get("tool_invocations", [])
    
    # Reset context pool for new research iteration
    if iteration == 1:
        reset_context_pool()
    
    # If no execution plan, fall back to sub-queries with web_search
    if not micro_tasks:
        logger.info("No execution plan, falling back to sub-query based web search")
        micro_tasks = [
            {"task_id": f"task_{i+1}", "description": sq, "tool": "web_search", 
             "inputs": {"query": sq}, "fallback_tools": ["news_search"], "persist": True}
            for i, sq in enumerate(sub_queries or [query])
        ]
    
    # 1. Hardware-Aware Scaling
    from shared.hardware.detector import detect_hardware
    from services.orchestrator.mcp.parallel_executor import ParallelExecutor, ToolCall
    from services.orchestrator.mcp.client import get_mcp_orchestrator
    
    hw_profile = detect_hardware()
    max_workers = hw_profile.optimal_workers()
    logger.info(f"ðŸš€ Hardware-Aware Dispatcher: Using {max_workers} parallel workers (RAM: {hw_profile.ram_available_gb:.1f}GB)")
    
    executor = ParallelExecutor(max_concurrent=max_workers)
    mcp_client = get_mcp_orchestrator()
    use_direct = len(mcp_client.tool_names) == 0
    
    # Tool registry for dynamic routing
    # ... [Keep imports and direct_tools definition] ...
    # Search tools
    from mcp_servers.search_server.tools.web_search import web_search_tool
    
    # Scraper tools
    from mcp_servers.scraper_server.tools.fetch_url import fetch_url_tool
    
    # Python execution tools
    from mcp_servers.python_server.tools.execute_code import execute_code_tool
    from mcp_servers.python_server.tools.dataframe_ops import dataframe_ops_tool
    from mcp_servers.python_server.tools.sql_query import sql_query_tool
    
    # Crawler tools
    from mcp_servers.crawler_server.server import web_crawler_tool, sitemap_parser_tool, link_extractor_tool
    
    # Browser agent tools
    from mcp_servers.browser_agent_server.server import human_like_search_tool, source_validator_tool, BrowserAgentServer
    
    direct_tools = {
        "web_search": web_search_tool,
        "news_search": web_search_tool,
        "human_search": human_like_search_tool,
        "scrape_url": fetch_url_tool,
        "fetch_url": fetch_url_tool,
        "fetch_data": lambda args: web_search_tool({"query": args.get("query", ""), "max_results": 10}),
        "web_crawler": web_crawler_tool,
        "sitemap_parser": sitemap_parser_tool,
        "link_extractor": link_extractor_tool,
        "multi_browse": lambda args: BrowserAgentServer()._handle_multi_browse(args),
        "source_validator": source_validator_tool,
        "dataframe_ops": dataframe_ops_tool,
        "sql_query": sql_query_tool,
        "run_python": execute_code_tool,
        "execute_code": execute_code_tool,
        "analyze_data": dataframe_ops_tool,
        "parse_document": execute_code_tool,
        # Map build_graph to simple python analysis if not available
        "build_graph": lambda args: execute_code_tool({"code": f"# Building graph for: {args.get('query', '')}\nprint('Graph built successfully.')"}),
    }
    
    def build_tool_inputs(tool_name: str, description: str, original_inputs: dict, collected_facts: list = None) -> dict:
        """Build proper inputs based on tool requirements."""
        ctx = get_context_pool()
        
        # PYTHON TOOLS
        if tool_name in ["run_python", "execute_code", "parse_document"]:
            if "code" in original_inputs: return original_inputs
            code = generate_fallback_code(description, collected_facts or [])
            return {"code": code}
        
        if tool_name in ["sql_query"]:
             # If input is just a description, we need to generate SQL
             if "query" in original_inputs and not "SELECT" in original_inputs["query"].upper():
                  # This is likely a natural language description, not SQL
                  # Fallback to python generation which can handle SQL logic or return code
                  code = generate_fallback_code(description, collected_facts or [])
                  return {"query": code}  # The tool expects 'query' but we might need to be smarter
             
             if "operation" in original_inputs or "query" in original_inputs: return original_inputs
             return None
        
        if tool_name in ["dataframe_ops", "analyze_data"]:
             if "operation" in original_inputs or "query" in original_inputs: return original_inputs
             return None
        
        # SCRAPING & CRAWLING
        if tool_name in ["scrape_url", "fetch_url", "sitemap_parser", "link_extractor"]:
            if "url" in original_inputs: return original_inputs
            return None
        
        if tool_name == "web_crawler":
            if "start_url" in original_inputs: return original_inputs
            crawl_url = ctx.get_url()
            if crawl_url: return {"start_url": crawl_url, "max_depth": 3, "max_pages": 50}
            return None

        # BROWSER TOOLS
        if tool_name == "human_search":
            if "query" in original_inputs: return {"query": original_inputs["query"], "max_sites": 10}
            return {"query": description, "max_sites": 10}
            
        if tool_name == "multi_browse":
            if "urls" in original_inputs: return original_inputs
            return None

        # SEARCH TOOLS (default)
        if tool_name in ["web_search", "news_search", "fetch_data"]:
            if "query" in original_inputs: return original_inputs
            return {"query": description, "max_results": 20}
            
        return original_inputs

    # 2. Unified Tool Handler
    async def unified_tool_handler(name: str, args: dict) -> Any:
        if use_direct and name in direct_tools:
            return await direct_tools[name](args)
        elif not use_direct:
            return await mcp_client.call_tool(name, args)
        else:
            raise ValueError(f"Tool {name} not available")

    # 3. Parallel Execution Loop
    completed = {inv["task_id"] for inv in tool_invocations if inv.get("success")}
    processed_this_run = set()
    
    while True:
        # Find ready tasks
        ready_tasks = []
        for task in micro_tasks:
            t_id = task.get("task_id")
            if t_id in completed or t_id in processed_this_run:
                continue
                
            # Check dependencies
            deps = task.get("depends_on", [])
            if all(d in completed for d in deps):
                ready_tasks.append(task)
        
        if not ready_tasks:
            break
            
        # Batch execute
        batch = ready_tasks[:max_workers * 2] # Allow slight over-subscription for IO wait
        processed_this_run.update(t.get("task_id") for t in batch)
        
        logger.info(f"\nâš¡ Batching {len(batch)} tasks...")
        
        # Prepare calls
        calls = []
        task_map = {} # index -> task
        
        for idx, task in enumerate(batch):
            t_name = task.get("tool", "web_search")
            t_desc = task.get("description", "")
            t_inputs = task.get("inputs", {})
            t_id = task.get("task_id")
            
            final_inputs = build_tool_inputs(t_name, t_desc, t_inputs, facts)
            
            # Handle input build failure (fallback immediately or skip)
            if final_inputs is None:
                # Try fallback
                fallback_tools = task.get("fallback_tools") or ["web_search"]
                t_name = fallback_tools[0]
                
                # Attempt to build inputs for fallback tool
                final_inputs = build_tool_inputs(t_name, t_desc, t_inputs, facts)
                
                if final_inputs is None:
                    # Ultimate fallback to web_search
                    t_name = "web_search"
                    final_inputs = {"query": t_desc, "max_results": 10}
            
            calls.append(ToolCall(tool_name=t_name, arguments=final_inputs))
            task_map[idx] = task
            
        # EXECUTE BATCH
        results = await executor.execute_batch(calls, unified_tool_handler)
        
        # Process results
        for idx, res in enumerate(results):
            task = task_map[idx]
            t_id = task.get("task_id")
            t_persist = task.get("persist", True)
            
            success = res.success
            error_msg = None
            content_text = ""
            
            if success and res.result and hasattr(res.result, "content"):
                for c in res.result.content:
                    if hasattr(c, "text"):
                        content_text += c.text
                
                # Verify content isn't error text
                if "error" in content_text.lower() and len(content_text) < 200:
                     # Soft error check
                     pass

            if success:
                # Extract Facts
                fact = {
                    "text": content_text[:1000],
                    "query": task.get("description", "")[:200],
                    "source": calls[idx].tool_name,
                    "task_id": t_id,
                    "persist": t_persist
                }
                facts.append(fact)
                
                # Extract Sources
                ctx = get_context_pool()
                extracted_urls = ctx.extract_urls_from_text(content_text)
                for url in extracted_urls[:10]:
                    sources.append({
                        "url": url,
                        "title": task.get("description", "")[:80],
                        "tool": calls[idx].tool_name,
                        "task_id": t_id
                    })
                
                completed.add(t_id)
                logger.info(f"   âœ… Task {t_id} completed via {calls[idx].tool_name}")
            else:
                # Handle hard failure
                logger.warning(f"   âŒ Task {t_id} failed: {res.result}")
                # We do NOT add to completed, so it won't satisfy dependencies. 
                # Ideally we should try fallbacks here, but for now we mark failed to avoid infinite loop
                tool_invocations.append({
                    "task_id": t_id,
                    "tool": calls[idx].tool_name,
                    "success": False,
                    "error": "Execution failed"
                })
                # Prevent re-execution to avoid infinite loop
                completed.add(t_id) 

            # Log invocation
            tool_invocations.append({
                 "task_id": t_id,
                 "tool": calls[idx].tool_name,
                 "success": success,
                 "persist": t_persist
            })

    # Summary Logging
    logger.info(f"\nðŸ“Š Parallel Research Summary:")
    logger.info(f"   Facts collected: {len(facts)}")
    logger.info(f"   Sources found: {len(sources)}")
    logger.info("="*70 + "\n")
    
    return {
        **state,
        "facts": facts,
        "sources": sources,
        "tool_invocations": tool_invocations,
    }


async def keeper_node(state: GraphState) -> GraphState:
    """Check for context drift and verify progress."""
    iteration = state.get("iteration", 0)
    max_iterations = state.get("max_iterations", 3)
    facts = state.get("facts", [])
    
    logger.info("\n" + "="*70)
    logger.info(f"ðŸ“ STEP 4: KEEPER NODE (Context Guard)")
    logger.info("="*70)
    logger.info(f"Iteration: {iteration}/{max_iterations}")
    logger.info(f"Facts collected: {len(facts)}")
    
    # Call real keeper that checks context drift
    updated_state = await real_keeper_node(dict(state))
    
    should_continue = updated_state.get("should_continue", False)
    drift_detected = updated_state.get("drift_detected", False)
    
    if drift_detected:
        logger.info("âš ï¸ Context drift detected!")
    
    if should_continue:
        logger.info("ðŸ”„ Decision: CONTINUE research loop")
    else:
        logger.info("âœ… Decision: STOP research, proceed to synthesis")
    
    logger.info("="*70 + "\n")
    
    return {**state, **updated_state}


async def generator_node(state: GraphState) -> GraphState:
    """Generate initial answer from facts using real LLM (Optimist role)."""
    logger.info("\n" + "="*70)
    logger.info("ðŸ“ STEP 5: GENERATOR NODE (LLM Call - The Optimist)")
    logger.info("="*70)
    
    facts = state.get("facts", [])
    sources = state.get("sources", [])
    query = state.get("query", "")
    
    logger.info(f"Query: {query[:100]}...")
    logger.info(f"Facts available: {len(facts)}")
    logger.info("-"*70)
    logger.info("Calling LLM to generate comprehensive answer...")
    
    generator = GeneratorAgent()
    output = await generator.generate(query, facts, sources)
    
    logger.info(f"\nâœ… Generator Output ({len(output)} chars):")
    logger.info("-"*70)
    # Print full output
    logger.info(output)
    logger.info("-"*70)
    logger.info("="*70 + "\n")
    
    return {
        **state,
        "generator_output": output,
    }


async def critic_node(state: GraphState) -> GraphState:
    """Critique the generator's output using real LLM (Pessimist role)."""
    logger.info("\n" + "="*70)
    logger.info("ðŸ“ STEP 6: CRITIC NODE (LLM Call - The Pessimist)")
    logger.info("="*70)
    
    generator_output = state.get("generator_output", "")
    facts = state.get("facts", [])
    sources = state.get("sources", [])
    
    logger.info(f"Reviewing answer ({len(generator_output)} chars)")
    logger.info("-"*70)
    logger.info("Calling LLM to critique and find weaknesses...")
    
    critic = CriticAgent()
    feedback = await critic.critique(generator_output, facts, sources)
    
    logger.info(f"\nâœ… Critic Feedback ({len(feedback)} chars):")
    logger.info("-"*70)
    # Print full feedback
    logger.info(feedback)
    logger.info("-"*70)
    logger.info("="*70 + "\n")
    
    return {
        **state,
        "critic_feedback": feedback,
    }


async def judge_node(state: GraphState) -> GraphState:
    """Synthesize generator/critic into final verdict using real LLM."""
    logger.info("\n" + "="*70)
    logger.info("ðŸ“ STEP 7: JUDGE NODE (LLM Call - The Arbiter)")
    logger.info("="*70)
    
    query = state.get("query", "")
    generator_output = state.get("generator_output", "")
    critic_feedback = state.get("critic_feedback", "")
    
    logger.info(f"Evaluating: Generator ({len(generator_output)} chars) vs Critic ({len(critic_feedback)} chars)")
    logger.info("-"*70)
    logger.info("Calling LLM to make final judgment...")
    
    judge = JudgeAgent()
    result = await judge.judge(query, generator_output, critic_feedback)
    
    verdict = result.get("verdict", "Accept")
    confidence = result.get("confidence", 0.5)
    reasoning = result.get("reasoning", "")
    
    logger.info(f"\nâœ… Judge Verdict: {verdict}")
    logger.info(f"âœ… Confidence: {confidence:.0%}")
    logger.info(f"\nðŸ“ Reasoning:")
    logger.info("-"*70)
    logger.info(reasoning[:500] if len(reasoning) > 500 else reasoning)
    logger.info("-"*70)
    logger.info("="*70 + "\n")
    
    return {
        **state,
        "judge_verdict": verdict,
        "confidence": confidence,
    }


async def synthesizer_node(state: GraphState) -> GraphState:
    """Create final report from all inputs."""
    logger.info("\n" + "="*70)
    logger.info("ðŸ“ STEP 8: SYNTHESIZER NODE (Final Report)")
    logger.info("="*70)
    
    query = state.get("query", "")
    facts = state.get("facts", [])
    sources = state.get("sources", [])
    generator_output = state.get("generator_output", "")
    confidence = state.get("confidence", 0.0)
    verdict = state.get("judge_verdict", "Unknown")
    
    logger.info(f"Query: {query[:100]}...")
    logger.info(f"Facts collected: {len(facts)}")
    logger.info(f"Sources: {len(sources)}")
    logger.info(f"Judge Verdict: {verdict}")
    logger.info(f"Confidence: {confidence:.0%}")
    logger.info("-"*70)
    logger.info("Generating final report...")
    
    report = f"# Research Report: {query}\n\n"
    report += f"**Confidence:** {confidence:.0%}\n"
    report += f"**Sources:** {len(sources)}\n"
    report += f"**Facts Extracted:** {len(facts)}\n"
    report += f"**Judge Verdict:** {verdict}\n\n"
    report += "---\n\n"
    report += generator_output
    report += "\n\n---\n\n"
    report += "## Facts Collected\n\n"
    
    for i, fact in enumerate(facts[:10], 1):
        if isinstance(fact, dict):
            report += f"{i}. {fact.get('text', str(fact))[:150]}...\n"
        else:
            report += f"{i}. {str(fact)[:150]}...\n"
    
    report += "\n## Sources\n\n"
    
    for src in sources[:10]:
        report += f"- [{src.get('title', 'Unknown')}]({src.get('url', '')})\n"
    
    logger.info(f"\nâœ… Final Report Generated ({len(report)} chars)")
    logger.info("="*70)
    logger.info("\n" + "ðŸ"*35)
    logger.info("                    RESEARCH COMPLETE")
    logger.info("ðŸ"*35 + "\n")
    
    return {
        **state,
        "report": report,
        "status": "completed",
    }


# ============================================================================
# Routing Functions
# ============================================================================

def should_continue_research(state: GraphState) -> Literal["researcher", "generator"]:
    """Decide whether to continue research or move to synthesis."""
    if state.get("should_continue", False):
        return "researcher"
    return "generator"


# ============================================================================
# Build Graph
# ============================================================================

def build_research_graph() -> StateGraph:
    """
    Build the LangGraph research state machine.
    
    Flow:
    router -> planner -> researcher -> keeper
                              ^            |
                              |            v
                              +-- (loop) --+
                                           |
                                           v
                              generator -> critic -> judge -> synthesizer -> END
    """
    graph = StateGraph(GraphState)
    
    # Add nodes
    graph.add_node(NodeName.ROUTER.value, router_node)
    graph.add_node(NodeName.PLANNER.value, planner_node)
    graph.add_node(NodeName.RESEARCHER.value, researcher_node)
    graph.add_node(NodeName.KEEPER.value, keeper_node)
    graph.add_node(NodeName.GENERATOR.value, generator_node)
    graph.add_node(NodeName.CRITIC.value, critic_node)
    graph.add_node(NodeName.JUDGE.value, judge_node)
    graph.add_node(NodeName.SYNTHESIZER.value, synthesizer_node)
    
    # Add edges
    graph.set_entry_point(NodeName.ROUTER.value)
    graph.add_edge(NodeName.ROUTER.value, NodeName.PLANNER.value)
    graph.add_edge(NodeName.PLANNER.value, NodeName.RESEARCHER.value)
    graph.add_edge(NodeName.RESEARCHER.value, NodeName.KEEPER.value)
    
    # Conditional edge: continue research or synthesize
    graph.add_conditional_edges(
        NodeName.KEEPER.value,
        should_continue_research,
        {
            "researcher": NodeName.RESEARCHER.value,
            "generator": NodeName.GENERATOR.value,
        }
    )
    
    # Consensus flow
    graph.add_edge(NodeName.GENERATOR.value, NodeName.CRITIC.value)
    graph.add_edge(NodeName.CRITIC.value, NodeName.JUDGE.value)
    graph.add_edge(NodeName.JUDGE.value, NodeName.SYNTHESIZER.value)
    graph.add_edge(NodeName.SYNTHESIZER.value, END)
    
    return graph


def compile_research_graph():
    """Compile the research graph for execution."""
    graph = build_research_graph()
    return graph.compile()
