{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Kea Research Engine - Simulation Notebook\n",
                "\n",
                "This notebook demonstrates the Kea research pipeline in a Kaggle/Colab environment.\n",
                "\n",
                "## Setup\n",
                "\n",
                "1. Set your `OPENROUTER_API_KEY` in the environment\n",
                "2. Run all cells to simulate a research flow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (Colab/Kaggle)\n",
                "!pip install -q httpx pydantic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import asyncio\n",
                "from datetime import datetime\n",
                "\n",
                "# Set your API key\n",
                "os.environ[\"OPENROUTER_API_KEY\"] = \"sk-or-your-key-here\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Test LLM Provider"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import httpx\n",
                "\n",
                "async def test_llm():\n",
                "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
                "    \n",
                "    async with httpx.AsyncClient(timeout=30) as client:\n",
                "        response = await client.post(\n",
                "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
                "            headers={\"Authorization\": f\"Bearer {api_key}\"},\n",
                "            json={\n",
                "                \"model\": \"nvidia/nemotron-3-nano-30b-a3b:free\",\n",
                "                \"messages\": [{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n",
                "                \"max_tokens\": 50,\n",
                "            }\n",
                "        )\n",
                "        response.raise_for_status()\n",
                "        data = response.json()\n",
                "    \n",
                "    print(\"LLM Response:\", data[\"choices\"][0][\"message\"][\"content\"])\n",
                "    return data\n",
                "\n",
                "await test_llm()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Simulate Fetch URL Tool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def simulate_fetch_url(url):\n",
                "    async with httpx.AsyncClient(timeout=30) as client:\n",
                "        response = await client.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
                "        response.raise_for_status()\n",
                "    \n",
                "    content = response.text[:5000]\n",
                "    print(f\"Fetched {len(response.text)} chars from {url}\")\n",
                "    return content\n",
                "\n",
                "result = await simulate_fetch_url(\"https://example.com\")\n",
                "print(result[:500])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Simulate Research Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dataclasses import dataclass, field\n",
                "from typing import List, Dict\n",
                "\n",
                "@dataclass\n",
                "class ResearchState:\n",
                "    query: str\n",
                "    sub_queries: List[str] = field(default_factory=list)\n",
                "    facts: List[Dict] = field(default_factory=list)\n",
                "    sources: List[Dict] = field(default_factory=list)\n",
                "    report: str = \"\"\n",
                "    iteration: int = 0\n",
                "\n",
                "async def router_node(state):\n",
                "    print(\"Router: Analyzing query...\")\n",
                "    return state\n",
                "\n",
                "async def planner_node(state):\n",
                "    print(\"Planner: Decomposing query...\")\n",
                "    state.sub_queries = [\n",
                "        f\"Latest data on {state.query}\",\n",
                "        f\"Key statistics for {state.query}\",\n",
                "    ]\n",
                "    return state\n",
                "\n",
                "async def researcher_node(state):\n",
                "    print(\"Researcher: Gathering data...\")\n",
                "    state.facts.append({\"entity\": state.query, \"value\": \"Sample data\"})\n",
                "    state.sources.append({\"url\": \"https://example.com\", \"title\": \"Sample\"})\n",
                "    state.iteration += 1\n",
                "    return state\n",
                "\n",
                "async def synthesizer_node(state):\n",
                "    print(\"Synthesizer: Creating report...\")\n",
                "    state.report = f\"# Report: {state.query}\\n\\nFound {len(state.facts)} facts.\"\n",
                "    return state\n",
                "\n",
                "async def run_pipeline(query):\n",
                "    print(f\"Starting Research: {query}\")\n",
                "    state = ResearchState(query=query)\n",
                "    state = await router_node(state)\n",
                "    state = await planner_node(state)\n",
                "    state = await researcher_node(state)\n",
                "    state = await synthesizer_node(state)\n",
                "    print(\"Research Complete!\")\n",
                "    return state\n",
                "\n",
                "state = await run_pipeline(\"Indonesia nickel production\")\n",
                "print(state.report)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Next Steps\n",
                "\n",
                "1. Add your OpenRouter API key to enable LLM-powered nodes\n",
                "2. Try different queries to test the pipeline\n",
                "3. Deploy with Docker using `docker-compose up`\n",
                "4. Connect to the API at `http://localhost:8080/docs`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}