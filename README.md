# Autonomous Multi-Agent Research Engine

**A high-throughput, microservice-based architecture for deep domain research, autonomous data analysis, and multi-agent consensus.**

## üìñ Technical Overview

This project is an **Autonomous Agentic System** designed to move beyond simple RAG (Retrieval Augmented Generation) towards **Deep Research**. It utilizes a hierarchical "Supervisor-Worker" architecture‚Äîa pattern where a central "Supervisor" LLM acts as an architect/planner, delegating specialized sub-tasks to "Worker" agents (specialized Python sub-systems).

Unlike standard chatbots, this system separates **Reasoning** (LLM) from **Execution** (Code). It features a self-correcting code execution environment, allowing agents to scrape data, build local SQL databases (DuckDB), perform statistical analysis via Pandas/Polars, and generate visualizations programmatically.

### Key Differentiators
*   **Microservices Architecture:** Fully containerized services communicating via REST/gRPC and OpenAPI standards.
*   **Graph-Based Orchestration:** Uses **LangGraph** for cyclic state management (loops, retries, consensus), moving beyond linear chains to dynamic, adaptive workflows.
*   **Code-First Analysis:** Hallucination reduction by forcing LLMs to write Python code for calculations rather than performing mental math.
*   **Asynchronous Parallelism:** Implements `asyncio` semaphores and batch processing to handle deep web scraping without hitting API rate limits.
*   **Persisted State:** Uses PostgreSQL checkpointers to support long-running research tasks (hours/days) with pause/resume capabilities.

---

## üèó System Architecture

The system follows a **Hub-and-Spoke** microservices pattern.

```mermaid
graph TD
    User["Client / API Consumer"] -->|REST Request| Gateway["API Gateway / Nginx"]
    
    subgraph "Service Layer"
        Gateway --> Orchestrator["Orchestrator Service<br>(LangGraph + FastAPI)"]
        
        Orchestrator -->|HTTP/RPC| ToolService["Tool Sandbox Service<br>(Docker/E2B)"]
        Orchestrator -->|HTTP/RPC| RAGService["Domain RAG Service<br>(Qdrant/Weaviate)"]
        Orchestrator -->|HTTP/RPC| ScraperService["Web Gathering Service<br>(Tavily/Firecrawl)"]
    end
    
    subgraph "Data Layer"
        Orchestrator -->|State Persist| Postgres[("PostgreSQL<br>Graph State")]
        ToolService -->|Analytical Data| DuckDB[("DuckDB<br>Parquet/CSV")]
        RAGService -->|Embeddings| VectorDB[("Vector DB")]
    end
```

### 1. The Orchestrator (The Brain)
*   **Stack:** Python, FastAPI, LangGraph, LiteLLM.
*   **Role:** Receives queries, decomposes them into a DAG (Directed Acyclic Graph) of tasks, and manages the state.
*   **Logic:** Decides *which* tool service to call based on OpenAPI specifications. Handles the "Consensus Loop" where critic agents review worker outputs.

### 2. The Tool Sandbox Service (The Muscle)
*   **Stack:** Docker / E2B Sandbox, Pandas, Scikit-Learn.
*   **Role:** Safely executes Python code generated by the Orchestrator in an isolated environment.
*   **Capabilities:**
    *   Dataframe manipulation (Filtering, Grouping, Correlation).
    *   Financial calculation (ratios, growth rates).
    *   Visualization generation (Plotly/Matplotlib).

### 3. The RAG Service (The Library)
*   **Stack:** LlamaIndex, Qdrant/Weaviate, Cross-Encoders.
*   **Role:** Domain-specific knowledge retrieval.
*   **Pipeline:** Hybrid Search (Keyword + Vector) ‚Üí Re-Ranking (Cohere/BGE) ‚Üí Context Injection.

---

## ‚öôÔ∏è The "Funnel" Workflow

The system utilizes a 3-stage funnel to optimize for cost and accuracy:

1.  **Phase 1: Broad Screening (Code-Heavy)**
    *   *Action:* Python scripts fetch broad datasets (e.g., 2,000 tickers or 500 legal cases).
    *   *Filtering:* Hard-coded filters (e.g., `PBV < 1.0` or `Year > 2020`) reduce the list to ~50 candidates.
    *   *No LLM:* This phase is purely deterministic to save tokens.

2.  **Phase 2: Deep Dive (Agent-Heavy)**
    *   *Action:* The 50 candidates are split into batches (e.g., 5 concurrent agents).
    *   *Parallelism:* Agents trigger sub-routines: News Search + Financial Extraction + Risk Analysis.
    *   *Throttling:* `asyncio.Semaphore` ensures external API rate limits are respected.

3.  **Phase 3: Synthesis & Consensus (Reasoning-Heavy)**
    *   *Action:* Aggregated reports are passed to a "Critic" agent.
    *   *Loop:* If data is missing or contradictory, the Critic rejects the state, triggering a targeted re-fetch loop.
    *   *Final Output:* HTML report generation.

---

### üß© System Architecture Diagram

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#252a34', 'edgeLabelBackground':'#ffffff', 'tertiaryColor': '#f5f5f5'}}}%%

graph TD
    %% --- STYLE DEFINITIONS ---
    classDef client fill:#ff6b6b,stroke:#333,stroke-width:2px,color:white;
    classDef gateway fill:#4ecdc4,stroke:#333,stroke-width:2px,color:white;
    classDef orchestrator fill:#45b7d1,stroke:#333,stroke-width:2px,color:white;
    classDef compute fill:#96ceb4,stroke:#333,stroke-width:2px,color:#333;
    classDef rag fill:#ffbe76,stroke:#333,stroke-width:2px,color:#333;
    classDef database fill:#2d3436,stroke:#333,stroke-width:2px,color:white,shape:cylinder;
    classDef external fill:#dfe6e9,stroke:#333,stroke-width:1px,color:#333,stroke-dasharray: 5 5;

    %% --- 1. CLIENT LAYER ---
    subgraph Clients ["üì° Client / Trigger Layer"]
        UI["üíª Web Dashboard"]:::client
        API_C["üîå API Consumer"]:::client
        Cron["‚è∞ Scheduled Cron"]:::client
    end

    %% --- 2. GATEWAY LAYER ---
    subgraph Gateway ["üõ°Ô∏è API Gateway & Load Balancer"]
        Nginx["NGINX / Kong Gateway"]:::gateway
        Auth["üîê Auth Service (JWT/OAuth)"]:::gateway
    end

    UI --> Nginx
    API_C --> Nginx
    Cron --> Nginx
    Nginx -.->|Validate Token| Auth

    %% --- 3. ORCHESTRATION SERVICE (THE BRAIN) ---
    subgraph Orchestrator_Cluster ["üß† Core Orchestration Service (LangGraph)"]
        direction TB
        
        API_Orch["FastAPI Endpoint"]:::orchestrator
        
        subgraph Graph_Logic ["State Machine (DAG)"]
            Planner["üìù Planner Node"]:::orchestrator
            Router["üîÄ Dispatcher/Router"]:::orchestrator
            
            subgraph Consensus_Loop ["üîÑ Consensus Sub-Architecture"]
                Critic["üßê Critic / Judge"]:::orchestrator
                Verifier["‚úÖ Fact Checker"]:::orchestrator
            end
            
            Writer["‚úçÔ∏è Report Generator"]:::orchestrator
        end
        
        Checkpointer["üíæ State Manager (Async)"]:::orchestrator
    end

    Nginx ==>|/research/trigger| API_Orch
    API_Orch --> Planner
    Planner --> Router
    
    %% FEEDBACK LOOPS
    Critic --"Reject (Contradiction)"--> Router
    Verifier --"Failed Verification"--> Router
    Critic --"Approved"--> Writer

    %% --- 4. DOMAIN AGENT SERVICES ---
    subgraph Agents ["üïµÔ∏è Specialized Agent Services"]
        direction LR
        
        subgraph Fin_Agent ["Finance Agent"]
            Fin_Logic["market_analyst.py"]:::compute
        end
        
        subgraph Legal_Agent ["Legal Agent"]
            Leg_Logic["legal_researcher.py"]:::compute
        end
        
        subgraph Web_Agent ["Deep Web Agent"]
            Scraper_Logic["scrapper.py"]:::compute
        end
    end

    Router == "gRPC/REST" ==> Fin_Agent
    Router == "gRPC/REST" ==> Legal_Agent
    Router == "gRPC/REST" ==> Web_Agent

    %% --- 5. TOOL SANDBOX (THE MUSCLE) ---
    subgraph Sandbox_Cluster ["üõ†Ô∏è Tool Sandbox Service (E2B / Docker)"]
        direction TB
        
        Code_Runner["üêç Python REPL (Isolated)"]:::compute
        
        subgraph Data_Science_Stack ["libs"]
            Pandas["Pandas/Polars"]:::compute
            Scikit["Scikit-Learn"]:::compute
            Matplot["Plotly/Matplotlib"]:::compute
        end
        
        DuckDB[("ü¶Ü DuckDB - In-Memory OLAP")]:::database
    end

    Fin_Agent -.->|Exec Code| Code_Runner
    Web_Agent -.->|Exec Scraper| Code_Runner
    Code_Runner --> Pandas
    Pandas --> DuckDB

    %% --- 6. RAG & KNOWLEDGE LAYER ---
    subgraph RAG_System ["üìö Universal RAG Service"]
        direction TB
        
        Ingest["üì• Ingestion Pipeline"]:::rag
        Retriever["üîé Hybrid Search"]:::rag
        Reranker["‚öñÔ∏è Cross-Encoder Rerank"]:::rag
        
        subgraph Embed_Models ["Model Serving"]
            BiEnc["Bi-Encoder Model"]:::rag
        end
    end

    Legal_Agent -.->|Query| Retriever
    Retriever --> BiEnc
    Retriever --> Reranker

    %% --- 7. PERSISTENCE LAYER ---
    subgraph Persistence ["üíæ Data Persistence Layer"]
        Postgres[("üêò PostgreSQL<br>Graph State & Users")]:::database
        VectorDB[("üí† Qdrant/Weaviate<br>Vector Store")]:::database
        Redis[("‚ö° Redis<br>Queue & Cache")]:::database
        S3[("üì¶ S3 / MinIO<br>Artifact Store")]:::database
    end

    Checkpointer --> Postgres
    RAG_System --> VectorDB
    API_Orch --> Redis
    Writer --> S3

    %% --- 8. EXTERNAL APIS ---
    subgraph External_World ["‚òÅÔ∏è External World"]
        LLM_Prov["ü§ñ LLM Providers<br>(OpenAI/Gemini/Anthropic)"]:::external
        Search_API["üåç Search APIs<br>(Tavily/Serp)"]:::external
        Fin_API["üìà Financial Data<br>(YFinance/OpenBB)"]:::external
    end

    Planner -.->|Inference| LLM_Prov
    Fin_Agent -.-> Fin_API
    Web_Agent -.-> Search_API
    
```

### üîç Architectural Explanation

This diagram is broken down into **Sub-Architectures** to show depth:

1.  **The Orchestration Cluster (The Brain):**
    *   Unlike simple scripts, this is a **State Machine**.
    *   **The Loop:** Notice the arrows from `Critic` back to `Router`. This represents the **Self-Correction** capability. If the data isn't good enough, the system loops back and tries again automatically.
    *   **Planner:** Breaks user input into atomic sub-tasks.

2.  **The Agent Layer (The Specialists):**
    *   These are decoupled Microservices. The `Finance Agent` is separate from the `Legal Agent`.
    *   This allows you to update the Financial logic without breaking the Legal logic.

3.  **The Tool Sandbox Service (The Muscle):**
    *   This is the most "Advanced" part. It is an **Isolated Sandbox** (Docker/E2B).
    *   **DuckDB Integration:** Agents don't just "read" text. They dump raw CSV/JSON data into an in-memory DuckDB instance to run SQL queries and complex Pandas aggregations. Use of DuckDB here is primarily for fast, in-process analytical queries on transient data, separate from the long-term persistence layer.

4.  **The RAG System (The Library):**
    *   Shows a full pipeline: `Ingest` ‚Üí `Hybrid Search` ‚Üí `Bi-Encoder` ‚Üí `Reranker`.
    *   This ensures high precision (Context Filtering) before data ever hits the LLM.

5.  **Data Persistence:**
    *   **PostgreSQL:** Saves the *thinking process* (LangGraph Checkpoints). You can pause a research job and resume it 2 days later.
    *   **S3/MinIO:** Stores the heavy artifacts (PDFs, Generated HTML Reports, Charts).
    *   **Redis:** Handles the message queue for long-running jobs (to prevent HTTP timeouts).
  
---
## üõ† Tech Stack

| Component | Technology | Reasoning |
| :--- | :--- | :--- |
| **Language** | Python 3.11+ | Native support for AI & Data Science libraries. |
| **API Framework** | FastAPI | High-performance, async support, auto-OpenAPI generation. |
| **Orchestration** | LangGraph | Supports cyclic graphs and fine-grained state control. |
| **LLM Interface** | LiteLLM | Unified interface for OpenAI, Anthropic, Gemini, & Local LLMs. |
| **Vector DB** | Qdrant / Weaviate | High-speed semantic search. |
| **Analytical DB** | DuckDB | In-process SQL OLAP database for handling agent-generated datasets. |
| **Search/Scraping** | Tavily / Firecrawl | LLM-optimized web data extraction. |
| **State DB** | PostgreSQL | Persistence for long-running graph threads. |
| **Containerization** | Docker Compose | Microservices orchestration. |

---

## ‚ö° Minimum Spec Execution Strategies

To run this high-throughput architecture on limited hardware (e.g., standard laptops with 8GB-16GB RAM, no dedicated GPU), follow these configuration strategies:

### 1. Cloud-First Inference
Avoid running local LLMs (like Llama-3-8b or Mistral). They consume significant RAM/VRAM.
*   **Strategy:** Configure `.env` to use exclusively cloud providers (OpenAI, Gemini, Anthropic) via LiteLLM.
*   **Benefit:** Offloads the heaviest computation (Generation & Reasoning) to the cloud.

### 2. Lightweight Vector Store
*   **Strategy:** Use **Qdrant** in `disk` mode (memory mapping) rather than full in-memory mode, or limit its container resource usage. Alternatively, use **ChromaDB** as a lighter-weight dev alternative if strictly necessary, though Qdrant is efficient.
*   **Docker Config:** Set `mem_limit: 512m` for the Vector DB container.

### 3. Database Optimizations
*   **PostgreSQL:** For local dev, a standard Postgres instance is fine, but you can turn off heavy logging or use **SQLite** (if supported by the specific persistence adapter) for extremely resource-constrained environments. *Recommendation: Stick to a lean Postgres container.*
*   **DuckDB:** Being in-process, this is already highly optimized. Ensure broad datasets fetched are capped (e.g., limit fetching 1000s of rows if RAM is tight).

### 4. Concurrency Limiting
Deep research tends to spawn multiple parallel agents.
*   **Strategy:** Limit `MAX_CONCURRENT_AGENTS=1` or `2` in your environment variables.
*   **Benefit:** Prevents the system from overwhelming your CPU/Network with too many concurrent Docker container spins or async tasks.

### 5. Sandbox Management
*   **Strategy:** Use **E2B** (Cloud Sandbox) instead of local Docker-in-Docker containers for the Tool Sandbox.
*   **Benefit:** Running code execution containers locally consumes significant overhead. E2B runs this in their cloud, saving your local machine resources.

---

## üöÄ Setup & Installation

### Prerequisites
*   Docker & Docker Compose
*   Python 3.11+
*   PostgreSQL (optional for local dev, required for prod)

### 1. Clone & Environment
```bash
git clone https://github.com/your-username/repo-name.git
cd repo-name
cp .env.example .env
```

### 2. Configuration (`.env`)
Fill in the required API keys. The system supports multi-provider routing.
```ini
# LLM Providers
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=...
ANTHROPIC_API_KEY=...

# Search & Tools
TAVILY_API_KEY=tvly-...
FIRECRAWL_API_KEY=fc-...

# Database
POSTGRES_URI=postgresql://user:pass@localhost:5432/agent_db
QDRANT_URL=http://localhost:6333
```

### 3. Run Microservices (Docker)
```bash
docker-compose up --build -d
```
This spins up:
*   `orchestrator-service` (Port 8000)
*   `rag-service` (Port 8001)
*   `tool-sandbox` (Port 8002)
*   `db-postgres` (Port 5432)
*   `db-qdrant` (Port 6333)

---

## üîå API Usage

The system exposes a RESTful API via FastAPI.

### Trigger Deep Research
**Endpoint:** `POST /api/v1/research/trigger`

```json
{
  "query": "Identify top 10 Indonesian mining companies with high export volume.",
  "domain": "finance",
  "depth": "deep",
  "config": {
    "max_concurrent_agents": 3,
    "require_quantitative_analysis": true,
    "output_format": "html"
  }
}
```

### Check Status (Long Polling)
**Endpoint:** `GET /api/v1/research/status/{thread_id}`

```json
{
  "thread_id": "550e8400-e29b...",
  "status": "processing",
  "current_step": "financial_analysis_agent",
  "completed_steps": ["planner", "screener", "data_collection"],
  "artifacts": {
    "candidates_csv": "s3://bucket/data/candidates.csv"
  }
}
```

---

## üìÇ Project Structure

```text
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator/       # Main LangGraph Logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents/         # Planner, Critic, Writer definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ graph.py        # Node & Edge definitions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py         # FastAPI entry point
‚îÇ   ‚îú‚îÄ‚îÄ rag_service/        # Embedding & Retrieval Logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ingestion.py    # PDF/Text ingestion pipelines
‚îÇ   ‚îî‚îÄ‚îÄ tool_sandbox/       # Python REPL & Sandbox
‚îÇ       ‚îî‚îÄ‚îÄ tools.py        # Pandas/Scikit scripts
‚îú‚îÄ‚îÄ shared/                 # Shared Pydantic models & Utils
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

## üó∫ Roadmap

*   [x] Core Microservices Architecture
*   [x] LangGraph Implementation (Cyclic)
*   [x] DuckDB Integration for Agent Analytics
*   [ ] **Model Context Protocol (MCP)** Implementation for IDE integration.
*   [ ] **Multimodal Agents:** Adding Whisper (Audio) and Vision processing nodes.
*   [ ] **Human-in-the-Loop:** UI for pausing execution and manual approval of "Consensus" steps.

## ü§ù Contribution

1.  Fork the repository.
2.  Create a feature branch (`git checkout -b feature/amazing-feature`).
3.  Commit your changes (`git commit -m 'Add some amazing feature'`).
4.  Push to the branch (`git push origin feature/amazing-feature`).
5.  Open a Pull Request.

## üìÑ License

Distributed under the MIT License. See `LICENSE` for more information.
